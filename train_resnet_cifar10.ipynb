{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPlH1w4Fdq1E",
        "outputId": "af98522c-11c9-44ce-ed9b-670103985974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/RobustBench/robustbench.git\n",
            "  Cloning https://github.com/RobustBench/robustbench.git to /tmp/pip-req-build-tqysbe1x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/RobustBench/robustbench.git /tmp/pip-req-build-tqysbe1x\n",
            "  Resolved https://github.com/RobustBench/robustbench.git to commit 2d630bc9e8d1cf50d46a4dda65fd36850e3ef769\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting autoattack@ git+https://github.com/fra31/auto-attack.git@a39220048b3c9f2cca9a4d3a54604793c68eca7e#egg=autoattack\n",
            "  Using cached autoattack-0.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (0.14.1+cu116)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (0.2.3)\n",
            "Requirement already satisfied: geotorch in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (0.3.0)\n",
            "Requirement already satisfied: requests~=2.25.0 in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (1.21.6)\n",
            "Requirement already satisfied: Jinja2~=3.1.2 in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (3.1.2)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (4.64.1)\n",
            "Requirement already satisfied: pandas~=1.3.5 in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (1.3.5)\n",
            "Requirement already satisfied: timm~=0.6.7 in /usr/local/lib/python3.8/dist-packages (from robustbench==1.1) (0.6.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2~=3.1.2->robustbench==1.1) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas~=1.3.5->robustbench==1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas~=1.3.5->robustbench==1.1) (2022.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests~=2.25.0->robustbench==1.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.25.0->robustbench==1.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.25.0->robustbench==1.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.25.0->robustbench==1.1) (1.24.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from timm~=0.6.7->robustbench==1.1) (0.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm~=0.6.7->robustbench==1.1) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.1->robustbench==1.1) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.8.2->robustbench==1.1) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from torchdiffeq->robustbench==1.1) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas~=1.3.5->robustbench==1.1) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm~=0.6.7->robustbench==1.1) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm~=0.6.7->robustbench==1.1) (23.0)\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/RobustBench/robustbench.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErufjhkIrXPC",
        "outputId": "c6dd6606-7f1b-4819-d272-257d2c476c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX13jqQxs0iH",
        "outputId": "135aecf6-de77-4988-aef0-d47873f7b675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV8Y5pHVGSE-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from robustbench.data import load_cifar10\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('drive/MyDrive')"
      ],
      "metadata": {
        "id": "vHgmd5QntB3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-35_B4JKlxY"
      },
      "source": [
        "# Load Cifar-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HukySLtXKpQK",
        "outputId": "0998baef-ed96-4dfd-cad5-5ebf6f1746a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "x_test, y_test = load_cifar10(n_examples=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCZS1s1CfVoB",
        "outputId": "be5a2927-3120-4c40-b26e-01d4ec25adc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(degrees=60),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "batch_size_test = 512\n",
        "sampler = SubsetRandomSampler(list(range(512)))\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         sampler=sampler, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox80wZL02jQK",
        "outputId": "261624b8-a91f-49ec-fcf0-e3c611d0b4e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRGlOYJ8f_F2"
      },
      "outputs": [],
      "source": [
        "for i,data in enumerate(testloader):\n",
        "  x_test_2, y_test_2 = data\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezs16vyJgRv9",
        "outputId": "729f0a1a-f9e1-44ec-c9f7-de231a74e2ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.6196, 0.6235, 0.6471,  ..., 0.5373, 0.4941, 0.4549],\n",
              "         [0.5961, 0.5922, 0.6235,  ..., 0.5333, 0.4902, 0.4667],\n",
              "         [0.5922, 0.5922, 0.6196,  ..., 0.5451, 0.5098, 0.4706],\n",
              "         ...,\n",
              "         [0.2667, 0.1647, 0.1216,  ..., 0.1490, 0.0510, 0.1569],\n",
              "         [0.2392, 0.1922, 0.1373,  ..., 0.1020, 0.1137, 0.0784],\n",
              "         [0.2118, 0.2196, 0.1765,  ..., 0.0941, 0.1333, 0.0824]],\n",
              "\n",
              "        [[0.4392, 0.4353, 0.4549,  ..., 0.3725, 0.3569, 0.3333],\n",
              "         [0.4392, 0.4314, 0.4471,  ..., 0.3725, 0.3569, 0.3451],\n",
              "         [0.4314, 0.4275, 0.4353,  ..., 0.3843, 0.3725, 0.3490],\n",
              "         ...,\n",
              "         [0.4863, 0.3922, 0.3451,  ..., 0.3804, 0.2510, 0.3333],\n",
              "         [0.4549, 0.4000, 0.3333,  ..., 0.3216, 0.3216, 0.2510],\n",
              "         [0.4196, 0.4118, 0.3490,  ..., 0.3020, 0.3294, 0.2627]],\n",
              "\n",
              "        [[0.1922, 0.1843, 0.2000,  ..., 0.1412, 0.1412, 0.1294],\n",
              "         [0.2000, 0.1569, 0.1765,  ..., 0.1216, 0.1255, 0.1333],\n",
              "         [0.1843, 0.1294, 0.1412,  ..., 0.1333, 0.1333, 0.1294],\n",
              "         ...,\n",
              "         [0.6941, 0.5804, 0.5373,  ..., 0.5725, 0.4235, 0.4980],\n",
              "         [0.6588, 0.5804, 0.5176,  ..., 0.5098, 0.4941, 0.4196],\n",
              "         [0.6275, 0.5843, 0.5176,  ..., 0.4863, 0.5059, 0.4314]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x_test[0,:,:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTXxEH8kgOoD",
        "outputId": "611cbf1a-04e8-4e09-d838-5d8934f7bf5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.9412, 0.9412, 0.9490,  ..., 0.7412, 0.7725, 0.7608],\n",
              "         [0.9373, 0.9373, 0.9216,  ..., 0.7294, 0.7333, 0.7176],\n",
              "         [0.8863, 0.9176, 0.8549,  ..., 0.8078, 0.7725, 0.7412],\n",
              "         ...,\n",
              "         [0.6706, 0.6863, 0.6941,  ..., 0.6902, 0.6902, 0.6902],\n",
              "         [0.6627, 0.6784, 0.6784,  ..., 0.6745, 0.6824, 0.6902],\n",
              "         [0.6392, 0.6471, 0.6549,  ..., 0.6588, 0.6667, 0.6588]],\n",
              "\n",
              "        [[0.9647, 0.9686, 0.9725,  ..., 0.7059, 0.7255, 0.7137],\n",
              "         [0.9686, 0.9725, 0.9569,  ..., 0.7529, 0.7490, 0.7333],\n",
              "         [0.9412, 0.9569, 0.8941,  ..., 0.7451, 0.7373, 0.7373],\n",
              "         ...,\n",
              "         [0.6627, 0.6784, 0.6784,  ..., 0.6000, 0.6078, 0.6039],\n",
              "         [0.6549, 0.6784, 0.6745,  ..., 0.5804, 0.5961, 0.6039],\n",
              "         [0.6392, 0.6471, 0.6549,  ..., 0.5529, 0.5647, 0.5686]],\n",
              "\n",
              "        [[0.9804, 0.9804, 0.9804,  ..., 0.7647, 0.7804, 0.7647],\n",
              "         [0.9843, 0.9804, 0.9804,  ..., 0.8353, 0.8235, 0.8118],\n",
              "         [0.9765, 0.9922, 0.9725,  ..., 0.8078, 0.7922, 0.8118],\n",
              "         ...,\n",
              "         [0.6431, 0.6627, 0.6706,  ..., 0.6314, 0.6392, 0.6431],\n",
              "         [0.6353, 0.6627, 0.6667,  ..., 0.6118, 0.6275, 0.6431],\n",
              "         [0.6314, 0.6431, 0.6510,  ..., 0.5843, 0.5961, 0.6000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "x_test_2[0,:,:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNZaouZ8LPQ_"
      },
      "outputs": [],
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL_erdWsKprL"
      },
      "source": [
        "# Define neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzgiVE6UHsCi"
      },
      "outputs": [],
      "source": [
        "__all__ = [\n",
        "    \"ResNet\",\n",
        "    \"resnet18\",\n",
        "    \"resnet34\",\n",
        "    \"resnet50\",\n",
        "]\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        num_classes=10,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        # END\n",
        "\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                self.groups,\n",
        "                self.base_width,\n",
        "                previous_dilation,\n",
        "                norm_layer,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, device, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        script_dir = os.path.dirname(__file__)\n",
        "        state_dict = torch.load(\n",
        "            script_dir + \"/state_dicts/\" + arch + \".pt\", map_location=device\n",
        "        )\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet(\n",
        "        \"resnet18\", BasicBlock, [2, 2, 2, 2], pretrained, progress, device, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet(\n",
        "        \"resnet34\", BasicBlock, [3, 4, 6, 3], pretrained, progress, device, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet(\n",
        "        \"resnet50\", Bottleneck, [3, 4, 6, 3], pretrained, progress, device, **kwargs\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSErJ7guJYz4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWXQixSqIuzq",
        "outputId": "b3adbbc9-cfe7-4dd5-9301-4be73b7bf011"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "my_model = resnet50(pretrained=False)\n",
        "my_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otgZTwzfiE_m",
        "outputId": "d7445ebb-3bba-4c27-f5ec-5afe1f6c7994"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaq1vsBlKwvm"
      },
      "source": [
        "# Define loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBhQfFm5KyIy"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(my_model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OXfOOFhCqyHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103b4c57-558e-4aea-cdf9-bacb8bb991af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZBTn0aKDpd"
      },
      "source": [
        "# Train the neural network on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b1OTy-eV-69"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = my_model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        # if (i+1) % 100 == 0:\n",
        "        #     last_loss = running_loss / 100 # loss per batch\n",
        "        #     print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        #     tb_x = epoch_index * len(trainloader) + i + 1\n",
        "        #     tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "        #     running_loss = 0.\n",
        "\n",
        "    return running_loss / (i+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Oiq1-7QWE6k",
        "outputId": "a076d54d-a47e-40c1-cb65-fd3764f3be26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "LOSS train 3.212356407014305 valid 16.9921875\n",
            "EPOCH 2:\n",
            "LOSS train 2.0702933661468195 valid 22.8515625\n",
            "EPOCH 3:\n",
            "LOSS train 1.904638250465588 valid 33.984375\n",
            "EPOCH 4:\n",
            "LOSS train 1.81426096907662 valid 36.328125\n",
            "EPOCH 5:\n",
            "LOSS train 1.7098523701548272 valid 42.3828125\n",
            "EPOCH 6:\n",
            "LOSS train 1.6319005675328053 valid 45.703125\n",
            "EPOCH 7:\n",
            "LOSS train 1.56918369504192 valid 29.8828125\n",
            "EPOCH 8:\n",
            "LOSS train 1.5168495199564473 valid 48.4375\n",
            "EPOCH 9:\n",
            "LOSS train 1.4700301534989302 valid 45.8984375\n",
            "EPOCH 10:\n",
            "LOSS train 1.4212930284802565 valid 41.2109375\n",
            "EPOCH 11:\n",
            "LOSS train 1.3921513215965018 valid 51.7578125\n",
            "EPOCH 12:\n",
            "LOSS train 1.3423283828798767 valid 40.234375\n",
            "EPOCH 13:\n",
            "LOSS train 1.3023828871719672 valid 41.40625\n",
            "EPOCH 14:\n",
            "LOSS train 1.2692576838881158 valid 42.578125\n",
            "EPOCH 15:\n",
            "LOSS train 1.2457139104833383 valid 51.171875\n",
            "EPOCH 16:\n",
            "LOSS train 1.2124490379677404 valid 49.8046875\n",
            "EPOCH 17:\n",
            "LOSS train 1.1914507435715718 valid 52.734375\n",
            "EPOCH 18:\n",
            "LOSS train 1.1635098420750454 valid 51.3671875\n",
            "EPOCH 19:\n",
            "LOSS train 1.142540422546894 valid 57.2265625\n",
            "EPOCH 20:\n",
            "LOSS train 1.1188484904406322 valid 54.6875\n",
            "EPOCH 21:\n",
            "LOSS train 1.098282856709512 valid 42.1875\n",
            "EPOCH 22:\n",
            "LOSS train 1.075851412990209 valid 59.375\n",
            "EPOCH 23:\n",
            "LOSS train 1.063047088625486 valid 62.109375\n",
            "EPOCH 24:\n",
            "LOSS train 1.0469754547109384 valid 64.453125\n",
            "EPOCH 25:\n",
            "LOSS train 1.0327346957553074 valid 59.1796875\n",
            "EPOCH 26:\n",
            "LOSS train 1.0242025018348109 valid 60.3515625\n",
            "EPOCH 27:\n",
            "LOSS train 1.0112421530896745 valid 47.265625\n",
            "EPOCH 28:\n",
            "LOSS train 0.9899742793853935 valid 63.4765625\n",
            "EPOCH 29:\n",
            "LOSS train 0.9757425445120048 valid 65.4296875\n",
            "EPOCH 30:\n",
            "LOSS train 0.9706715686851756 valid 49.8046875\n",
            "EPOCH 31:\n",
            "LOSS train 0.9670449827638123 valid 52.9296875\n",
            "EPOCH 32:\n",
            "LOSS train 0.9564061529191253 valid 65.8203125\n",
            "EPOCH 33:\n",
            "LOSS train 0.9445643650601282 valid 63.671875\n",
            "EPOCH 34:\n",
            "LOSS train 0.9392575612458427 valid 54.6875\n",
            "EPOCH 35:\n",
            "LOSS train 0.9312377177236025 valid 48.6328125\n",
            "EPOCH 36:\n",
            "LOSS train 0.9203660049097008 valid 64.0625\n",
            "EPOCH 37:\n",
            "LOSS train 0.9168550698348629 valid 65.4296875\n",
            "EPOCH 38:\n",
            "LOSS train 0.9092331957024382 valid 64.0625\n",
            "EPOCH 39:\n",
            "LOSS train 0.9085759727851205 valid 71.09375\n",
            "EPOCH 40:\n",
            "LOSS train 0.9009875527123357 valid 62.3046875\n",
            "EPOCH 41:\n",
            "LOSS train 0.8934994097560873 valid 64.0625\n",
            "EPOCH 42:\n",
            "LOSS train 0.8886029327007205 valid 62.5\n",
            "EPOCH 43:\n",
            "LOSS train 0.8804904626458502 valid 61.9140625\n",
            "EPOCH 44:\n",
            "LOSS train 0.8799591416593098 valid 71.484375\n",
            "EPOCH 45:\n",
            "LOSS train 0.8750902133829453 valid 62.109375\n",
            "EPOCH 46:\n",
            "LOSS train 0.8638929117975942 valid 67.3828125\n",
            "EPOCH 47:\n",
            "LOSS train 0.8623162322032177 valid 66.015625\n",
            "EPOCH 48:\n",
            "LOSS train 0.8566213135829057 valid 59.5703125\n",
            "EPOCH 49:\n",
            "LOSS train 0.8553050756454468 valid 56.8359375\n",
            "EPOCH 50:\n",
            "LOSS train 0.8521993873674242 valid 69.3359375\n",
            "EPOCH 51:\n",
            "LOSS train 0.8385677240083894 valid 54.8828125\n",
            "EPOCH 52:\n",
            "LOSS train 0.8363615007656614 valid 69.53125\n",
            "EPOCH 53:\n",
            "LOSS train 0.8369602632644536 valid 68.5546875\n",
            "EPOCH 54:\n",
            "LOSS train 0.8309258551853697 valid 66.796875\n",
            "EPOCH 55:\n",
            "LOSS train 0.8220030157767293 valid 66.40625\n",
            "EPOCH 56:\n",
            "LOSS train 0.8243814406492521 valid 62.5\n",
            "EPOCH 57:\n",
            "LOSS train 0.8131490883315006 valid 68.359375\n",
            "EPOCH 58:\n",
            "LOSS train 0.8104255929627382 valid 63.4765625\n",
            "EPOCH 59:\n",
            "LOSS train 0.8103650794614612 valid 72.0703125\n",
            "EPOCH 60:\n",
            "LOSS train 0.8055802240700978 valid 67.578125\n",
            "EPOCH 61:\n",
            "LOSS train 0.7983760146228859 valid 64.2578125\n",
            "EPOCH 62:\n",
            "LOSS train 0.7959576829924913 valid 73.828125\n",
            "EPOCH 63:\n",
            "LOSS train 0.7894171360508561 valid 61.71875\n",
            "EPOCH 64:\n",
            "LOSS train 0.7880689584080826 valid 65.625\n",
            "EPOCH 65:\n",
            "LOSS train 0.7781375645066771 valid 69.53125\n",
            "EPOCH 66:\n",
            "LOSS train 0.7796166191625473 valid 65.4296875\n",
            "EPOCH 67:\n",
            "LOSS train 0.7766863976598091 valid 71.09375\n",
            "EPOCH 68:\n",
            "LOSS train 0.7678068281744447 valid 70.8984375\n",
            "EPOCH 69:\n",
            "LOSS train 0.7690114154840064 valid 62.5\n",
            "EPOCH 70:\n",
            "LOSS train 0.7622555678762744 valid 61.9140625\n",
            "EPOCH 71:\n",
            "LOSS train 0.7623006541405797 valid 73.2421875\n",
            "EPOCH 72:\n",
            "LOSS train 0.7611028939256888 valid 68.9453125\n",
            "EPOCH 73:\n",
            "LOSS train 0.7469896134513113 valid 66.9921875\n",
            "EPOCH 74:\n",
            "LOSS train 0.7461280203841226 valid 68.75\n",
            "EPOCH 75:\n",
            "LOSS train 0.7382361526074617 valid 71.2890625\n",
            "EPOCH 76:\n",
            "LOSS train 0.7355580679748369 valid 75.1953125\n",
            "EPOCH 77:\n",
            "LOSS train 0.7319936618932983 valid 70.1171875\n",
            "EPOCH 78:\n",
            "LOSS train 0.7305819614768942 valid 66.6015625\n",
            "EPOCH 79:\n",
            "LOSS train 0.7242314294933359 valid 74.0234375\n",
            "EPOCH 80:\n",
            "LOSS train 0.7164184190428166 valid 61.328125\n",
            "EPOCH 81:\n",
            "LOSS train 0.7190251990657328 valid 57.8125\n",
            "EPOCH 82:\n",
            "LOSS train 0.7107813939109178 valid 66.796875\n",
            "EPOCH 83:\n",
            "LOSS train 0.7072713494758167 valid 69.3359375\n",
            "EPOCH 84:\n",
            "LOSS train 0.697500371109799 valid 73.6328125\n",
            "EPOCH 85:\n",
            "LOSS train 0.7018016300847768 valid 72.4609375\n",
            "EPOCH 86:\n",
            "LOSS train 0.6892016438571998 valid 61.9140625\n",
            "EPOCH 87:\n",
            "LOSS train 0.6910206876752322 valid 73.4375\n",
            "EPOCH 88:\n",
            "LOSS train 0.6838512206473923 valid 73.4375\n",
            "EPOCH 89:\n",
            "LOSS train 0.6819551045937307 valid 76.5625\n",
            "EPOCH 90:\n",
            "LOSS train 0.6743037327171286 valid 71.6796875\n",
            "EPOCH 91:\n",
            "LOSS train 0.6695843812297372 valid 70.3125\n",
            "EPOCH 92:\n",
            "LOSS train 0.6607247744222431 valid 67.3828125\n",
            "EPOCH 93:\n",
            "LOSS train 0.664375663108533 valid 69.3359375\n",
            "EPOCH 94:\n",
            "LOSS train 0.6494149174684148 valid 71.875\n",
            "EPOCH 95:\n",
            "LOSS train 0.6505042701730948 valid 65.234375\n",
            "EPOCH 96:\n",
            "LOSS train 0.6440284592111397 valid 69.921875\n",
            "EPOCH 97:\n",
            "LOSS train 0.6400231196904731 valid 76.5625\n",
            "EPOCH 98:\n",
            "LOSS train 0.6384319032701995 valid 79.296875\n",
            "EPOCH 99:\n",
            "LOSS train 0.6305098023713397 valid 70.8984375\n",
            "EPOCH 100:\n",
            "LOSS train 0.6289310826517432 valid 71.484375\n",
            "EPOCH 101:\n",
            "LOSS train 0.623000482997626 valid 71.875\n",
            "EPOCH 102:\n",
            "LOSS train 0.6117072016991618 valid 77.9296875\n",
            "EPOCH 103:\n",
            "LOSS train 0.6152171564224126 valid 72.0703125\n",
            "EPOCH 104:\n",
            "LOSS train 0.6008404474276716 valid 76.7578125\n",
            "EPOCH 105:\n",
            "LOSS train 0.6025662284982783 valid 75.5859375\n",
            "EPOCH 106:\n",
            "LOSS train 0.5959046691122567 valid 77.34375\n",
            "EPOCH 107:\n",
            "LOSS train 0.5877274418883311 valid 79.1015625\n",
            "EPOCH 108:\n",
            "LOSS train 0.5882807220796795 valid 82.8125\n",
            "Saving..\n",
            "EPOCH 109:\n",
            "LOSS train 0.579439072352846 valid 77.734375\n",
            "EPOCH 110:\n",
            "LOSS train 0.5809406099264579 valid 74.0234375\n",
            "EPOCH 111:\n",
            "LOSS train 0.5671288967894776 valid 77.9296875\n",
            "EPOCH 112:\n",
            "LOSS train 0.5637158488526064 valid 75.0\n",
            "EPOCH 113:\n",
            "LOSS train 0.5610275484259476 valid 72.8515625\n",
            "EPOCH 114:\n",
            "LOSS train 0.5584540448682692 valid 72.8515625\n",
            "EPOCH 115:\n",
            "LOSS train 0.5498577323563568 valid 78.125\n",
            "EPOCH 116:\n",
            "LOSS train 0.5516044430415649 valid 74.8046875\n",
            "EPOCH 117:\n",
            "LOSS train 0.5387915004702175 valid 71.09375\n",
            "EPOCH 118:\n",
            "LOSS train 0.5351553068441504 valid 78.515625\n",
            "EPOCH 119:\n",
            "LOSS train 0.5273775802091565 valid 76.953125\n",
            "EPOCH 120:\n",
            "LOSS train 0.5268787715746008 valid 75.390625\n",
            "EPOCH 121:\n",
            "LOSS train 0.5159193833008446 valid 81.25\n",
            "EPOCH 122:\n",
            "LOSS train 0.5109938306881644 valid 79.6875\n",
            "EPOCH 123:\n",
            "LOSS train 0.5050851491558582 valid 77.9296875\n",
            "EPOCH 124:\n",
            "LOSS train 0.5056191227015328 valid 79.8828125\n",
            "EPOCH 125:\n",
            "LOSS train 0.49500286792550247 valid 80.078125\n",
            "EPOCH 126:\n",
            "LOSS train 0.4964536731810216 valid 81.0546875\n",
            "EPOCH 127:\n",
            "LOSS train 0.48587645517895595 valid 80.859375\n",
            "EPOCH 128:\n",
            "LOSS train 0.4823062789562108 valid 81.25\n",
            "EPOCH 129:\n",
            "LOSS train 0.4749358768963143 valid 82.8125\n",
            "EPOCH 130:\n",
            "LOSS train 0.470934571131416 valid 79.6875\n",
            "EPOCH 131:\n",
            "LOSS train 0.4583161894775108 valid 78.3203125\n",
            "EPOCH 132:\n",
            "LOSS train 0.45647741431165534 valid 86.328125\n",
            "Saving..\n",
            "EPOCH 133:\n",
            "LOSS train 0.44788303003286767 valid 80.859375\n",
            "EPOCH 134:\n",
            "LOSS train 0.4507159660844242 valid 84.765625\n",
            "EPOCH 135:\n",
            "LOSS train 0.44206644125911587 valid 86.5234375\n",
            "Saving..\n",
            "EPOCH 136:\n",
            "LOSS train 0.433691673533386 valid 85.7421875\n",
            "EPOCH 137:\n",
            "LOSS train 0.42975008689686467 valid 83.59375\n",
            "EPOCH 138:\n",
            "LOSS train 0.41988904309242275 valid 84.375\n",
            "EPOCH 139:\n",
            "LOSS train 0.4122463345451428 valid 84.375\n",
            "EPOCH 140:\n",
            "LOSS train 0.40760281529572917 valid 83.3984375\n",
            "EPOCH 141:\n",
            "LOSS train 0.4046717873772087 valid 82.2265625\n",
            "EPOCH 142:\n",
            "LOSS train 0.3927022961094556 valid 86.1328125\n",
            "EPOCH 143:\n",
            "LOSS train 0.39233581264458045 valid 84.765625\n",
            "EPOCH 144:\n",
            "LOSS train 0.3853896857070191 valid 86.9140625\n",
            "Saving..\n",
            "EPOCH 145:\n",
            "LOSS train 0.3762203284617885 valid 83.203125\n",
            "EPOCH 146:\n",
            "LOSS train 0.36969987343034477 valid 84.5703125\n",
            "EPOCH 147:\n",
            "LOSS train 0.3612667732607678 valid 83.3984375\n",
            "EPOCH 148:\n",
            "LOSS train 0.3576058577698515 valid 83.984375\n",
            "EPOCH 149:\n",
            "LOSS train 0.3511638592957231 valid 86.71875\n",
            "EPOCH 150:\n",
            "LOSS train 0.34269078323603286 valid 87.3046875\n",
            "Saving..\n",
            "EPOCH 151:\n",
            "LOSS train 0.3365557705197493 valid 87.5\n",
            "Saving..\n",
            "EPOCH 152:\n",
            "LOSS train 0.3294539558689308 valid 84.9609375\n",
            "EPOCH 153:\n",
            "LOSS train 0.3267348653367718 valid 84.765625\n",
            "EPOCH 154:\n",
            "LOSS train 0.31397933823525753 valid 82.2265625\n",
            "EPOCH 155:\n",
            "LOSS train 0.3029060715909504 valid 86.71875\n",
            "EPOCH 156:\n",
            "LOSS train 0.29799326804592785 valid 88.671875\n",
            "Saving..\n",
            "EPOCH 157:\n",
            "LOSS train 0.29271340156760056 valid 89.453125\n",
            "Saving..\n",
            "EPOCH 158:\n",
            "LOSS train 0.28645250995826843 valid 88.671875\n",
            "EPOCH 159:\n"
          ]
        }
      ],
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 200\n",
        "\n",
        "best_vacc = 80\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    my_model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "    # We don't need gradients on to do reporting\n",
        "    my_model.train(False)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    vcorrect = 0\n",
        "    total = 0\n",
        "    for i, vdata in enumerate(testloader):\n",
        "        vinputs, vlabels = vdata\n",
        "        vinputs = vinputs.to(device)\n",
        "        vlabels = vlabels.to(device)\n",
        "        voutputs = my_model(vinputs)\n",
        "        vloss = criterion(voutputs, vlabels)\n",
        "        running_vloss += vloss\n",
        "        _, vpredicted = torch.max(voutputs,1)\n",
        "        total += vlabels.size(0)\n",
        "        vcorrect += (vlabels == vpredicted).sum().item()\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    vacc = (100 * vcorrect / total)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, vacc  ))\n",
        "\n",
        "    if vacc > best_vacc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': my_model.state_dict(),\n",
        "            'acc': vacc,\n",
        "            'epoch': EPOCHS,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt_resnet50.pth')\n",
        "        best_vacc = vacc\n",
        "    \n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    # if avg_vloss < best_vloss:\n",
        "    #     best_vloss = avg_vloss\n",
        "    #     model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "    #     torch.save(my_model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6TljAzK7AU"
      },
      "source": [
        "# Test the neural network on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn_Ea-z9NDI3"
      },
      "outputs": [],
      "source": [
        "batch_size_test = 512\n",
        "#sampler = SubsetRandomSampler(list(range(512)))\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGjiyvj0QAvm"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = my_model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 1024 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okxr-yiIx9q6"
      },
      "outputs": [],
      "source": [
        "model_path = 'model_{}_{}'.format(256, EPOCHS)\n",
        "torch.save(my_model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKiUubGNySh1"
      },
      "outputs": [],
      "source": [
        "# my_model = resnet50(pretrained= False)\n",
        "# my_model.load_state_dict(torch.load(model_path))\n",
        "# my_model.to(device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}